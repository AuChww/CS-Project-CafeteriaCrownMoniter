# import torch
# import cv2
# import numpy as np
# from ultralytics import YOLO

# # à¹‚à¸«à¸¥à¸” YOLOv8 pre-trained model
# model = YOLO('yolov8s.pt')  # à¹ƒà¸Šà¹‰à¹‚à¸¡à¹€à¸”à¸¥ YOLOv8

# # à¹€à¸›à¸´à¸”à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸à¸¥à¹‰à¸­à¸‡
# cap = cv2.VideoCapture(0)

# # à¸à¸³à¸«à¸™à¸”à¸žà¸·à¹‰à¸™à¸—à¸µà¹ˆà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸™à¸±à¸š (ROI)
# roi_top_left = (50, 50)  # à¸ˆà¸¸à¸”à¸¡à¸¸à¸¡à¸šà¸™à¸‹à¹‰à¸²à¸¢
# roi_bottom_right = (1500, 1500)  # à¸ˆà¸¸à¸”à¸¡à¸¸à¸¡à¸¥à¹ˆà¸²à¸‡à¸‚à¸§à¸²

# # à¸•à¸±à¸§à¹à¸›à¸£à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¹ƒà¸™ ROI
# human_count = 0
# detected_humans = []  # à¸•à¸±à¸§à¹à¸›à¸£à¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸­à¸‡à¸„à¸™à¸—à¸µà¹ˆà¸•à¸£à¸§à¸ˆà¸žà¸š

# # à¸•à¸±à¸§à¹à¸›à¸£à¹€à¸à¹‡à¸š bounding box à¸‚à¸­à¸‡à¸„à¸™à¹ƒà¸™à¹€à¸Ÿà¸£à¸¡à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²
# previous_people = []

# # à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸„à¸™à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸žà¸·à¹‰à¸™à¸—à¸µà¹ˆ ROI à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
# def is_within_roi(left, top, right, bottom, roi_top_left, roi_bottom_right):
#     return (left > roi_top_left[0] and right < roi_bottom_right[0] and
#             top > roi_top_left[1] and bottom < roi_bottom_right[1])

# while True:
#     ret, frame = cap.read()
#     if not ret:
#         break

#     results = model(frame)
#     frame_h, frame_w, _ = frame.shape
#     detections = results[0].boxes.xyxy.cpu().numpy()  # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ bounding boxes
#     classes = results[0].boxes.cls.cpu().numpy()     # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ class
#     human_count = 0
#     current_people = []

#     for i, box in enumerate(detections):
#         left, top, right, bottom = map(int, box[:4])
#         cls = int(classes[i])

#         if cls == 0 and is_within_roi(left, top, right, bottom, roi_top_left, roi_bottom_right):  # à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹€à¸‰à¸žà¸²à¸°à¸„à¸™
#             current_people.append((left, top, right, bottom))  # à¹€à¸à¹‡à¸š bounding box à¸‚à¸­à¸‡à¸„à¸™à¹ƒà¸™à¹€à¸Ÿà¸£à¸¡à¸™à¸µà¹‰
#             cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
#             cv2.putText(frame, f"Person", (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

#     # à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¹ƒà¸«à¸¡à¹ˆà¸—à¸µà¹ˆà¹€à¸žà¸´à¹ˆà¸¡à¹€à¸‚à¹‰à¸²à¸¡à¸²à¹ƒà¸™ ROI
#     for person in current_people:
#         if person not in previous_people:
#             human_count += 1

#     previous_people = current_people  # à¸­à¸±à¸›à¹€à¸”à¸• bounding box à¸‚à¸­à¸‡à¹€à¸Ÿà¸£à¸¡à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™à¹„à¸›à¸¢à¸±à¸‡à¸•à¸±à¸§à¹à¸›à¸£ previous_people

#     # à¹à¸ªà¸”à¸‡à¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¸—à¸µà¹ˆà¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹„à¸”à¹‰
#     cv2.putText(frame, f"Human Count: {human_count}", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)
#     cv2.rectangle(frame, roi_top_left, roi_bottom_right, (0, 255, 0), 2)  # à¸§à¸²à¸” ROI

#     cv2.imshow('YOLOv8 Object Detection - Human Count', frame)

#     if cv2.waitKey(1) & 0xFF == ord('q'):
#         break


# # à¸›à¸´à¸”à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸à¸¥à¹‰à¸­à¸‡
# cap.release()
# cv2.destroyAllWindows()



# ----------------------------------------------------------------------------------------------
# With blur all body


# import torch
# import cv2
# import numpy as np
# from ultralytics import YOLO

# # à¹‚à¸«à¸¥à¸” YOLOv8 pre-trained model
# model = YOLO('yolov8s.pt')  # à¹ƒà¸Šà¹‰à¹‚à¸¡à¹€à¸”à¸¥ YOLOv8

# # à¹€à¸›à¸´à¸”à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸à¸¥à¹‰à¸­à¸‡
# cap = cv2.VideoCapture(0)

# # à¸à¸³à¸«à¸™à¸”à¸žà¸·à¹‰à¸™à¸—à¸µà¹ˆà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸™à¸±à¸š (ROI)
# roi_top_left = (50, 50)  # à¸ˆà¸¸à¸”à¸¡à¸¸à¸¡à¸šà¸™à¸‹à¹‰à¸²à¸¢
# roi_bottom_right = (1500, 1500)  # à¸ˆà¸¸à¸”à¸¡à¸¸à¸¡à¸¥à¹ˆà¸²à¸‡à¸‚à¸§à¸²

# # à¸•à¸±à¸§à¹à¸›à¸£à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¹ƒà¸™ ROI
# human_count = 0
# detected_humans = []  # à¸•à¸±à¸§à¹à¸›à¸£à¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸­à¸‡à¸„à¸™à¸—à¸µà¹ˆà¸•à¸£à¸§à¸ˆà¸žà¸š

# # à¸•à¸±à¸§à¹à¸›à¸£à¹€à¸à¹‡à¸š bounding box à¸‚à¸­à¸‡à¸„à¸™à¹ƒà¸™à¹€à¸Ÿà¸£à¸¡à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²
# previous_people = []

# # à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸„à¸™à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸žà¸·à¹‰à¸™à¸—à¸µà¹ˆ ROI à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
# def is_within_roi(left, top, right, bottom, roi_top_left, roi_bottom_right):
#     return (left > roi_top_left[0] and right < roi_bottom_right[0] and
#             top > roi_top_left[1] and bottom < roi_bottom_right[1])

# while True:
#     ret, frame = cap.read()
#     if not ret:
#         break

#     results = model(frame)
#     frame_h, frame_w, _ = frame.shape
#     detections = results[0].boxes.xyxy.cpu().numpy()  # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ bounding boxes
#     classes = results[0].boxes.cls.cpu().numpy()     # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ class
#     human_count = 0
#     current_people = []

#     for i, box in enumerate(detections):
#         left, top, right, bottom = map(int, box[:4])
#         cls = int(classes[i])

#         if cls == 0 and is_within_roi(left, top, right, bottom, roi_top_left, roi_bottom_right):  # à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹€à¸‰à¸žà¸²à¸°à¸„à¸™
#             current_people.append((left, top, right, bottom))  # à¹€à¸à¹‡à¸š bounding box à¸‚à¸­à¸‡à¸„à¸™à¹ƒà¸™à¹€à¸Ÿà¸£à¸¡à¸™à¸µà¹‰
            
#             # à¹€à¸šà¸¥à¸­à¹ƒà¸šà¸«à¸™à¹‰à¸²à¹ƒà¸™ Bounding Box
#             face_region = frame[top:bottom, left:right]
#             blurred_face = cv2.GaussianBlur(face_region, (51, 51), 30)
#             frame[top:bottom, left:right] = blurred_face
            
#             cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
#             cv2.putText(frame, f"Person", (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

#     # à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¹ƒà¸«à¸¡à¹ˆà¸—à¸µà¹ˆà¹€à¸žà¸´à¹ˆà¸¡à¹€à¸‚à¹‰à¸²à¸¡à¸²à¹ƒà¸™ ROI
#     for person in current_people:
#         if person not in previous_people:
#             human_count += 1

#     previous_people = current_people  # à¸­à¸±à¸›à¹€à¸”à¸• bounding box à¸‚à¸­à¸‡à¹€à¸Ÿà¸£à¸¡à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™à¹„à¸›à¸¢à¸±à¸‡à¸•à¸±à¸§à¹à¸›à¸£ previous_people

#     # à¹à¸ªà¸”à¸‡à¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¸—à¸µà¹ˆà¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹„à¸”à¹‰
#     cv2.putText(frame, f"Human Count: {human_count}", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)
#     cv2.rectangle(frame, roi_top_left, roi_bottom_right, (0, 255, 0), 2)  # à¸§à¸²à¸” ROI

#     cv2.imshow('YOLOv8 Object Detection - Human Count with Privacy', frame)

#     if cv2.waitKey(1) & 0xFF == ord('q'):
#         break

# # à¸›à¸´à¸”à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸à¸¥à¹‰à¸­à¸‡
# cap.release()
# cv2.destroyAllWindows()
# ----------------------------------------------------------------------------------------------

# Face blur

# import torch
# import cv2
# import numpy as np
# import os
# from ultralytics import YOLO

# # à¹‚à¸«à¸¥à¸” YOLOv8 pre-trained model
# model = YOLO('yoloModel/yolov8s.pt')  # à¹ƒà¸Šà¹‰à¹‚à¸¡à¹€à¸”à¸¥ YOLOv8

# # à¹‚à¸«à¸¥à¸” Haar Cascade à¸ªà¸³à¸«à¸£à¸±à¸šà¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹ƒà¸šà¸«à¸™à¹‰à¸²
# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# # # à¹€à¸›à¸´à¸”à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸à¸¥à¹‰à¸­à¸‡
# # cap = cv2.VideoCapture(0)


# # à¹€à¸›à¸´à¸”à¸§à¸´à¸”à¸µà¹‚à¸­à¸”à¹‰à¸§à¸¢ OpenCV
# def get_human_count(zone_id):
    
#     # à¸ªà¸£à¹‰à¸²à¸‡ path à¹„à¸›à¸¢à¸±à¸‡à¹„à¸Ÿà¸¥à¹Œà¸§à¸´à¸”à¸µà¹‚à¸­
#     zone_id = f"vid_zone_{zone_id}.mp4"
#     video_path = os.path.join(os.path.dirname(__file__), "../public/video/{zone_id}")
#     # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹„à¸Ÿà¸¥à¹Œà¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¸ˆà¸£à¸´à¸‡à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
#     if not os.path.exists(video_path):
#         print(f"Video file for zone {zone_id} not found.")
#         return 0  # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸§à¸´à¸”à¸µà¹‚à¸­ à¹ƒà¸«à¹‰à¸„à¸·à¸™à¸„à¹ˆà¸² 0 à¹„à¸›à¹€à¸¥à¸¢
    
#     cap = cv2.VideoCapture(video_path)


#     # à¸à¸³à¸«à¸™à¸”à¸žà¸·à¹‰à¸™à¸—à¸µà¹ˆà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸™à¸±à¸š (ROI)
#     roi_top_left = (50, 50)  # à¸ˆà¸¸à¸”à¸¡à¸¸à¸¡à¸šà¸™à¸‹à¹‰à¸²à¸¢
#     roi_bottom_right = (1500, 1500)  # à¸ˆà¸¸à¸”à¸¡à¸¸à¸¡à¸¥à¹ˆà¸²à¸‡à¸‚à¸§à¸²

#     # à¸•à¸±à¸§à¹à¸›à¸£à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¹ƒà¸™ ROI
#     human_count = 0
#     previous_people = []
#     frame_number = 0

#     # à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸„à¸™à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸žà¸·à¹‰à¸™à¸—à¸µà¹ˆ ROI à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
#     def is_within_roi(left, top, right, bottom, roi_top_left, roi_bottom_right):
#         return (left > roi_top_left[0] and right < roi_bottom_right[0] and
#                 top > roi_top_left[1] and bottom < roi_bottom_right[1])
        
        
#     all_human_counts = [];

#     while True:
#         ret, frame = cap.read()
#         if not ret:
#             break
        
#         frame_number += 1

#         results = model(frame)
#         frame_h, frame_w, _ = frame.shape
#         detections = results[0].boxes.xyxy.cpu().numpy()  # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ bounding boxes
#         classes = results[0].boxes.cls.cpu().numpy()     # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ class
#         human_count = 0
#         current_people = []

#         for i, box in enumerate(detections):
#             left, top, right, bottom = map(int, box[:4])
#             cls = int(classes[i])

#             if cls == 0 and is_within_roi(left, top, right, bottom, roi_top_left, roi_bottom_right):  # à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹€à¸‰à¸žà¸²à¸°à¸„à¸™
#                 current_people.append((left, top, right, bottom))  # à¹€à¸à¹‡à¸š bounding box à¸‚à¸­à¸‡à¸„à¸™à¹ƒà¸™à¹€à¸Ÿà¸£à¸¡à¸™à¸µà¹‰

#                 # Crop à¹€à¸‰à¸žà¸²à¸°à¸šà¸£à¸´à¹€à¸§à¸“à¸„à¸™à¸ˆà¸²à¸à¹€à¸Ÿà¸£à¸¡
#                 person_region = frame[top:bottom, left:right]

#                 # à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹ƒà¸šà¸«à¸™à¹‰à¸²à¹ƒà¸™ Bounding Box
#                 gray_person = cv2.cvtColor(person_region, cv2.COLOR_BGR2GRAY)
#                 faces = face_cascade.detectMultiScale(gray_person, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

#                 for (fx, fy, fw, fh) in faces:
#                     # à¹€à¸šà¸¥à¸­à¹€à¸‰à¸žà¸²à¸°à¹ƒà¸šà¸«à¸™à¹‰à¸²
#                     face_region = person_region[fy:fy+fh, fx:fx+fw]
#                     blurred_face = cv2.GaussianBlur(face_region, (51, 51), 30)
#                     person_region[fy:fy+fh, fx:fx+fw] = blurred_face

#                 # à¹à¸›à¸°à¸à¸¥à¸±à¸šà¹„à¸›à¸¢à¸±à¸‡à¹€à¸Ÿà¸£à¸¡à¸«à¸¥à¸±à¸
#                 frame[top:bottom, left:right] = person_region

#                 # à¸§à¸²à¸” Bounding Box à¸£à¸­à¸šà¸„à¸™
#                 cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
#                 cv2.putText(frame, "Person", (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

#         # à¸™à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¹ƒà¸«à¸¡à¹ˆà¸—à¸µà¹ˆà¹€à¸žà¸´à¹ˆà¸¡à¹€à¸‚à¹‰à¸²à¸¡à¸²à¹ƒà¸™ ROI
#         for person in current_people:
#             if person not in previous_people:
#                 human_count += 1

#         previous_people = current_people  # à¸­à¸±à¸›à¹€à¸”à¸• bounding box à¸‚à¸­à¸‡à¹€à¸Ÿà¸£à¸¡à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™à¹„à¸›à¸¢à¸±à¸‡à¸•à¸±à¸§à¹à¸›à¸£ previous_people
        
#         all_human_counts.append(human_count)

#         # à¹à¸ªà¸”à¸‡à¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¸—à¸µà¹ˆà¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹„à¸”à¹‰
#         cv2.putText(frame, f"Human Count: {human_count}", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)
        
        
#         cv2.rectangle(frame, roi_top_left, roi_bottom_right, (0, 0, 255), 2)  # à¸§à¸²à¸” ROI

#         # cv2.imshow('YOLOv8 Object Detection - Human Count & Face Blur', frame)

#         if cv2.waitKey(1) & 0xFF == ord('q'):
#             break
        
#         # print("Human counts in all frames:", all_human_counts)
        
#         if frame_number % 60 == 0:  # à¸žà¸´à¸¡à¸žà¹Œà¸—à¸¸à¸ 60 à¹€à¸Ÿà¸£à¸¡
#             print("Human counts in all frames:", all_human_counts)
        
        
#         # test = sum(all_human_counts)
        
#         # print(test)
#         # cap.release()
#     return all_human_counts[-1] if all_human_counts else 0
    
# # à¸›à¸´à¸”à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¸à¸¥à¹‰à¸­à¸‡
# # cap.release()
# cv2.destroyAllWindows()

# ----------------------------------------------------------------------------------------------------------------------------------------------------------
# Roi with 2 zone 

# import torch
# import cv2
# import numpy as np
# import os
# from ultralytics import YOLO

# # à¹‚à¸«à¸¥à¸” YOLOv8 pre-trained model
# model = YOLO('yoloModel/yolov8s.pt')  # à¹ƒà¸Šà¹‰à¹‚à¸¡à¹€à¸”à¸¥ YOLOv8

# # à¹‚à¸«à¸¥à¸” Haar Cascade à¸ªà¸³à¸«à¸£à¸±à¸šà¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹ƒà¸šà¸«à¸™à¹‰à¸²
# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# # à¹€à¸›à¸´à¸”à¸§à¸´à¸”à¸µà¹‚à¸­à¸”à¹‰à¸§à¸¢ OpenCV
# def get_human_count(zone_id):
#     # à¸ªà¸£à¹‰à¸²à¸‡ path à¹„à¸›à¸¢à¸±à¸‡à¹„à¸Ÿà¸¥à¹Œà¸§à¸´à¸”à¸µà¹‚à¸­
#     zone_id = "vidva.mp4"
#     # zone_id = f"vid_zone_{zone_id}.mp4"
#     # video_path = os.path.join(os.path.dirname(__file__), f"../public/video/{zone_id}")
#     video_path = os.path.join(os.path.dirname(__file__), f"../public/video/{zone_id}")
    
#     if not os.path.exists(video_path):
#         print(f"Video file for zone {zone_id} not found.")
#         return 0  # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸§à¸´à¸”à¸µà¹‚à¸­ à¹ƒà¸«à¹‰à¸„à¸·à¸™à¸„à¹ˆà¸² 0
    
#     cap = cv2.VideoCapture(video_path)

#     # à¸à¸³à¸«à¸™à¸” ROI (Region of Interest) à¸ªà¸­à¸‡à¸žà¸·à¹‰à¸™à¸—à¸µà¹ˆ
#     # area ROI zone 1
#     # top left (50,50)
#     # bottom right (100,700)
#     # width = 100 - 50 = 50
#     # height = 700 - 50 = 650
#     # area = 50 * 650 = 32,500 pixels
#     roi_areas = {
#                 # à¹€à¸£à¸´à¹ˆà¸¡à¸¡à¸¸à¸¡à¸‹à¹‰à¸²à¸¢à¸šà¸™,à¹€à¸£à¸´à¹ˆà¸¡à¸¡à¸¸à¸¡à¸‹à¹‰à¸²à¸¢à¸¥à¹ˆà¸²à¸‡
#         "Zone 1": [(50, 50), (100, 700)],   # ROI 1 
#         "Zone 2": [(800, 50), (1500, 700)]  # ROI 2
#     }

#     frame_number = 0
#     human_counts = {zone: [] for zone in roi_areas}  # à¹€à¸à¹‡à¸šà¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸° ROI

#     def is_within_roi(left, top, right, bottom, roi_top_left, roi_bottom_right):
#         return (left > roi_top_left[0] and right < roi_bottom_right[0] and
#                 top > roi_top_left[1] and bottom < roi_bottom_right[1])

#     while True:
#         ret, frame = cap.read()
#         if not ret:
#             break
        
#         frame_number += 1
#         results = model(frame)
#         detections = results[0].boxes.xyxy.cpu().numpy()
#         classes = results[0].boxes.cls.cpu().numpy()
        
#         current_counts = {zone: 0 for zone in roi_areas}
        
#         for i, box in enumerate(detections):
#             left, top, right, bottom = map(int, box[:4])
#             cls = int(classes[i])

#             if cls == 0:  # à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹€à¸‰à¸žà¸²à¸°à¸„à¸™
#                 for zone, (roi_top_left, roi_bottom_right) in roi_areas.items():
#                     if is_within_roi(left, top, right, bottom, roi_top_left, roi_bottom_right):
#                         current_counts[zone] += 1
                        
#                         # à¸§à¸²à¸” Bounding Box
#                         cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
#                         cv2.putText(frame, "Person", (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                        
#                         # Crop à¹€à¸‰à¸žà¸²à¸°à¸šà¸£à¸´à¹€à¸§à¸“à¸„à¸™à¸ˆà¸²à¸à¹€à¸Ÿà¸£à¸¡
#                         person_region = frame[top:bottom, left:right]
#                         gray_person = cv2.cvtColor(person_region, cv2.COLOR_BGR2GRAY)
#                         faces = face_cascade.detectMultiScale(gray_person, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
                        
#                         for (fx, fy, fw, fh) in faces:
#                             face_region = person_region[fy:fy+fh, fx:fx+fw]
#                             blurred_face = cv2.GaussianBlur(face_region, (51, 51), 30)
#                             person_region[fy:fy+fh, fx:fx+fw] = blurred_face
                        
#                         frame[top:bottom, left:right] = person_region
                        
#         # à¸šà¸±à¸™à¸—à¸¶à¸à¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸° ROI
#         for zone in roi_areas:
#             human_counts[zone].append(current_counts[zone])

#         # à¸§à¸²à¸” ROI à¹à¸¥à¸°à¹à¸ªà¸”à¸‡à¸ˆà¸³à¸™à¸§à¸™à¸„à¸™
#         for zone, (roi_top_left, roi_bottom_right) in roi_areas.items():
#             cv2.rectangle(frame, roi_top_left, roi_bottom_right, (0, 0, 255), 2)
#             cv2.putText(frame, f"{zone}: {current_counts[zone]}", (roi_top_left[0], roi_top_left[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

#         cv2.imshow('YOLOv8 Human Detection', frame)
        
#         cv2.waitKey(0) 
        
#         if cv2.waitKey(1) & 0xFF == ord('q'):
#             break
        
    
#     cap.release()
#     cv2.destroyAllWindows()
    
#     return {zone: human_counts[zone][-1] if human_counts[zone] else 0 for zone in roi_areas}

# get_human_count(1)


# ----------------------------------------------------------------------------------------------------------------------------------------------------------
# Faster Version

import torch
import cv2
import numpy as np
import os
from ultralytics import YOLO

# à¹‚à¸«à¸¥à¸” YOLOv8 pre-trained model
model = YOLO('yoloModel/yolov8s.pt') 

# à¹€à¸›à¸´à¸”à¸§à¸´à¸”à¸µà¹‚à¸­à¸”à¹‰à¸§à¸¢ OpenCV
def get_human_count(zone_id):
    # à¸ªà¸£à¹‰à¸²à¸‡ path à¹„à¸›à¸¢à¸±à¸‡à¹„à¸Ÿà¸¥à¹Œà¸§à¸´à¸”à¸µà¹‚à¸­
    # zone_id = "{zone_id}.mp4"
    video_path = os.path.join(os.path.dirname(__file__), f"../public/video/{zone_id}.mp4")
    print(f"start human count zone {zone_id}")
    
    if not os.path.exists(video_path):
        print(f"Video file {zone_id} not found.")
        return 0  
    
    cap = cv2.VideoCapture(video_path)

    # à¸à¸³à¸«à¸™à¸” ROI (Region of Interest)
    roi_areas = {
        "Zone 1": [(50, 50), (700, 700)],  
        "Restaurant Zone": [(800, 50), (1500, 700)]
    }

    frame_number = 0
    human_counts = {zone: [] for zone in roi_areas}

    def is_within_roi(left, top, right, bottom, roi_top_left, roi_bottom_right):
        return (left > roi_top_left[0] and right < roi_bottom_right[0] and
                top > roi_top_left[1] and bottom < roi_bottom_right[1])
    
    print(f"before while loop")

    while True:
        print(f"in while loop")
        ret, frame = cap.read()
        if not ret:
            break

        frame_number += 1

        # à¸‚à¹‰à¸²à¸¡à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸šà¸²à¸‡à¹€à¸Ÿà¸£à¸¡ à¹€à¸žà¸·à¹ˆà¸­à¸¥à¸”à¸ à¸²à¸£à¸° (à¹€à¸Šà¹ˆà¸™ à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸—à¸¸à¸ 5 à¹€à¸Ÿà¸£à¸¡)
        if frame_number % 5 != 0:
            continue

        # ðŸ”» à¸¥à¸”à¸‚à¸™à¸²à¸”à¸ à¸²à¸žà¹ƒà¸«à¹‰ YOLO à¸—à¸³à¸‡à¸²à¸™à¹€à¸£à¹‡à¸§à¸‚à¸¶à¹‰à¸™
        small_frame = cv2.resize(frame, (640, 360))

        results = model(small_frame)
        detections = results[0].boxes.xyxy.cpu().numpy()
        classes = results[0].boxes.cls.cpu().numpy()

        print(f"before current count")

        current_counts = {zone: 0 for zone in roi_areas}

        for i, box in enumerate(detections):
            left, top, right, bottom = map(int, box[:4])
            cls = int(classes[i])

            if cls == 0:  # à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹€à¸‰à¸žà¸²à¸°à¸„à¸™
                for zone, (roi_top_left, roi_bottom_right) in roi_areas.items():
                    if is_within_roi(left, top, right, bottom, roi_top_left, roi_bottom_right):
                        current_counts[zone] += 1

        print(f"before roi in while")

        # à¸šà¸±à¸™à¸—à¸¶à¸à¸ˆà¸³à¸™à¸§à¸™à¸„à¸™à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸° ROI
        for zone in roi_areas:
            human_counts[zone].append(current_counts[zone])

        print(f"before mod")

        # à¹à¸ªà¸”à¸‡à¸œà¸¥à¹à¸„à¹ˆà¸—à¸¸à¸à¹† 10 à¹€à¸Ÿà¸£à¸¡
        if frame_number % 10 == 0:
            for zone, (roi_top_left, roi_bottom_right) in roi_areas.items():
                cv2.rectangle(frame, roi_top_left, roi_bottom_right, (0, 0, 255), 2)
                cv2.putText(frame, f"{zone}: {current_counts[zone]}", (roi_top_left[0], roi_top_left[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

            # cv2.imshow('YOLOv8 Human Detection', frame)

        print(f"before break")

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    print(f"before print roi area")

    cap.release()
    cv2.destroyAllWindows()
    
    for zone in roi_areas:
        print(f"zone {zone}: {human_counts[zone][-1] if human_counts[zone] else 0}")

    print(f"success human count")

    return {zone : human_counts[zone][-1] if human_counts[zone] else 0 for zone in roi_areas}
